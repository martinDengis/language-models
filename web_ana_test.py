# -*- coding: utf-8 -*-
"""web_ana_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rszNOT_CIhAJ9ag34Q1Jm2SS29RQtueU
"""

# Install required packages with specific versions
"""!pip install torch==2.1.0 torchtext==0.16.0 torchvision==0.16.0
!pip install numpy==1.23.5 pandas==2.0.1 matplotlib==3.7.1 tqdm==4.66.3
!pip install spacy  # Add this line
!python -m spacy download fr_core_news_sm  # Add this line

# Restart runtime after installation (you'll need to run this manually in Colab)
import os
os.kill(os.getpid(), 9)

from google.colab import drive
drive.mount('/content/drive') """

import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch import optim
from torch.utils.data import DataLoader, Dataset
import torch.nn.functional as F

from torchtext.vocab import build_vocab_from_iterator

from tqdm import trange, tqdm
import time

import warnings
warnings.filterwarnings('ignore')

# Check if GPU is available
print("CUDA available:", torch.cuda.is_available())
# Check if MPS is available
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print(f"Using device: MPS")
elif torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"Using device: CUDA")
else:
    device = torch.device("cpu")
    print(f"Using device: CPU")

# Define the hyperparameters
learning_rate = 1e-4
nepochs = 10
batch_size = 64
max_len = 8
hidden_size = 8
num_layers = 1

# Adapt corpus path to your drive hierarchy
corpus_path = 'cleaned_lemonde_corpus.txt'

# Custom Dataset for LeMonde corpus
class LemondeDataset(Dataset):
    """
    A custom dataset class for processing text data from a given file, tokenizing it, and creating sequences for language modeling tasks.

    Attributes:
        sequence_length (int): The length of each sequence.
        data (torch.Tensor): The tensor containing token indices.
        sequences (list): A list of sequences created from the token indices.
        targets (list): A list of target sequences corresponding to the input sequences.

    Methods:
        __len__(): Returns the number of sequences in the dataset.
        __getitem__(idx): Returns the sequence and target at the specified index.

    Args:
        file_path (str): The path to the text file containing the corpus.
        vocab (dict): A dictionary mapping tokens to their corresponding indices.
        tokenizer (callable): A function or callable object that tokenizes the text.
        sequence_length (int): The length of each sequence to be created.
    """
    def __init__(self, file_path, vocab, tokenizer, sequence_length):
        self.sequence_length = sequence_length

        # Read the corpus
        print("Reading corpus from:", file_path)
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        print(f"Corpus length: {len(text)} characters")

        # Tokenize the entire text
        print("Tokenizing text...")
        tokens = tokenizer(text)
        print(f"Number of tokens: {len(tokens)}")

        # Convert tokens to indices
        print("Converting tokens to indices...")
        self.data = torch.tensor([vocab[token] for token in tokens], dtype=torch.long)

        # Create sequences and targets
        print("Creating sequences...")
        self.sequences = []
        self.targets = []

        for i in range(0, len(self.data) - sequence_length):
            sequence = self.data[i:i + sequence_length]
            target = self.data[i + 1:i + sequence_length + 1]
            self.sequences.append(sequence)
            self.targets.append(target)

        print(f"Created {len(self.sequences)} sequences")

    def __len__(self):
        """Returns the number of sequences in the object."""
        return len(self.sequences)

    def __getitem__(self, idx):
        """Retrieve the sequence and target at the specified index."""
        return self.sequences[idx], self.targets[idx]
    
import spacy
from tqdm import tqdm

def create_french_tokenizer():
    """Create and return a French tokenizer function"""
    try:
        # Load spaCy model with disabled components for speed
        nlp = spacy.load('fr_core_news_sm', disable=['parser', 'ner'])

        def tokenize_text(text, chunk_size=900000):
            tokens = []
            # Split text into chunks
            chunks = [text[i:i + chunk_size]
                     for i in range(0, len(text), chunk_size)]

            print(f"Processing {len(chunks)} chunks...")

            # Process each chunk
            for chunk in tqdm(chunks):
                doc = nlp(chunk)
                chunk_tokens = [token.text for token in doc]
                tokens.extend(chunk_tokens)

            return tokens

        return tokenize_text

    except OSError:
        print("Installing French language model...")
        import os
        os.system('python -m spacy download fr_core_news_sm')
        nlp = spacy.load('fr_core_news_sm', disable=['parser', 'ner'])
        return create_french_tokenizer()
    
# Tokenizer and Vocabulary Creation
print("Initializing tokenizer...")
tokenizer = create_french_tokenizer()

# Read corpus for vocab creation
print(f"Reading corpus from {corpus_path}...")
with open(corpus_path, 'r', encoding='utf-8') as f:
    corpus = f.read()

def yield_tokens(text):
    yield tokenizer(text)

# Build vocabulary
print("Building vocabulary...")
vocab = build_vocab_from_iterator(
    yield_tokens(corpus),
    min_freq=2,
    specials=['<pad>', '<sos>', '<eos>', '<unk>'],
    special_first=True
)
vocab.set_default_index(vocab['<unk>'])
print(f"Vocabulary size: {len(vocab)}")
    
# Dataset Creation and Splitting
print("Creating dataset...")
dataset = LemondeDataset(corpus_path, vocab, tokenizer, max_len)

print("Splitting dataset...")
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)

# LSTM Model Definition
class LSTM(nn.Module):
    def __init__(self, num_emb, output_size, num_layers=1, hidden_size=128):
        super(LSTM, self).__init__()

        self.embedding = nn.Embedding(num_emb, hidden_size)
        self.lstm = nn.LSTM(input_size=hidden_size,
                           hidden_size=hidden_size,
                           num_layers=num_layers,
                           batch_first=True,
                           dropout=0.5)
        self.fc_out = nn.Linear(hidden_size, output_size)

    def forward(self, input_seq, hidden_in, mem_in):
        input_embs = self.embedding(input_seq)
        output, (hidden_out, mem_out) = self.lstm(input_embs, (hidden_in, mem_in))
        return self.fc_out(output), hidden_out, mem_out

def generate_text(model, vocab, tokenizer, seed_text, max_length=20):
    """
    Generate text using the trained model (LSTM or GRU)

    Args:
        model: The trained model (LSTM or GRU)
        vocab: Vocabulary object
        tokenizer: Tokenizer function
        seed_text: Initial text to start generation
        max_length: Maximum number of tokens to generate

    Returns:
        str: The generated text
    """
    model.eval()
    device = next(model.parameters()).device

    # Tokenize seed text
    tokens = tokenizer(seed_text)
    current_sequence = torch.tensor([vocab[token] for token in tokens], dtype=torch.long)

    # Initialize hidden states
    hidden = torch.zeros(num_layers, 1, hidden_size, device=device)

    # Initialize memory cell only for LSTM
    memory = torch.zeros(num_layers, 1, hidden_size, device=device) if isinstance(model, LSTM) else None

    generated_tokens = []

    with torch.no_grad():
        for _ in range(max_length):
            # Prepare input
            input_sequence = current_sequence[-1:].unsqueeze(0).to(device)

            # Get model prediction based on model type
            if isinstance(model, LSTM):
                output, hidden, memory = model(input_sequence, hidden, memory)
            else:  # GRU case
                output, hidden = model(input_sequence, hidden)

            # Get probabilities and sample next token
            probs = F.softmax(output.squeeze(), dim=-1)
            next_token_idx = torch.multinomial(probs, 1).item()

            # Append to generated sequence
            current_sequence = torch.cat([current_sequence, torch.tensor([next_token_idx])])

            # Get the actual token
            for token, idx in vocab.get_stoi().items():
                if idx == next_token_idx:
                    generated_tokens.append(token)
                    break

    return seed_text + ' ' + ' '.join(generated_tokens)

def calculate_perplexity(loss):
    """Calculate perplexity from loss"""
    return torch.exp(torch.tensor(loss)).item()

# LSTM Model Definition
class LSTM(nn.Module):
    """
    A Long Short-Term Memory (LSTM) neural network module.

    Args:
        num_emb (int): The size of the input vocabulary.
        output_size (int): The size of the output layer.
        num_layers (int, optional): The number of LSTM layers. Default is 1.
        hidden_size (int, optional): The number of features in the hidden state. Default is 128.

    Attributes:
        embedding (nn.Embedding): Embedding layer that converts input indices to dense vectors.
        lstm (nn.LSTM): LSTM layer(s) for processing sequences.
        fc_out (nn.Linear): Fully connected layer that maps LSTM outputs to the desired output size.

    Methods:
        forward(input_seq, hidden_in, mem_in):
            Performs a forward pass of the LSTM network.

            Args:
                input_seq (Tensor): Input sequence tensor of shape (batch_size, seq_length).
                hidden_in (Tensor): Initial hidden state tensor of shape (num_layers, batch_size, hidden_size).
                mem_in (Tensor): Initial cell state tensor of shape (num_layers, batch_size, hidden_size).

            Returns:
                Tuple[Tensor, Tensor, Tensor]: Output tensor of shape (batch_size, seq_length, output_size),
                                               hidden state tensor of shape (num_layers, batch_size, hidden_size),
                                               and cell state tensor of shape (num_layers, batch_size, hidden_size).
    """
    def __init__(self, num_emb, output_size, num_layers=1, hidden_size=128):
        super(LSTM, self).__init__()

        self.embedding = nn.Embedding(num_emb, hidden_size)
        self.lstm = nn.LSTM(input_size=hidden_size,
                           hidden_size=hidden_size,
                           num_layers=num_layers,
                           batch_first=True,
                           dropout=0.5)
        self.fc_out = nn.Linear(hidden_size, output_size)

    def forward(self, input_seq, hidden_in, mem_in):
        input_embs = self.embedding(input_seq)
        output, (hidden_out, mem_out) = self.lstm(input_embs, (hidden_in, mem_in))
        return self.fc_out(output), hidden_out, mem_out

import glob

# File path for the checkpoint directory
checkpoint_dir = "checkpoints/"
checkpoint_pattern = f"{checkpoint_dir}/checkpoint_epoch_*.pth"

# Find the latest checkpoint
checkpoint_files = glob.glob(checkpoint_pattern)
if checkpoint_files:
    # Sort by epoch number extracted from the file name
    checkpoint_files.sort(key=lambda x: int(x.split('_epoch_')[-1].split('.pth')[0]))
    checkpoint_path = checkpoint_files[-1]  # Get the latest checkpoint file
    print(f"Checkpoint found: {checkpoint_path}")

    # Load checkpoint
    checkpoint = torch.load(checkpoint_path)

    # Initialize model with the same architecture
    model = LSTM(num_emb=len(vocab),
                 output_size=len(vocab),
                 num_layers=num_layers,
                 hidden_size=hidden_size).to(device)

    # Load saved model and optimizer states
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    # Optionally, load epoch and losses to resume training
    start_epoch = checkpoint['epoch'] + 1
    train_losses = checkpoint['train_losses']
    test_losses = checkpoint['test_losses']
    perplexities = checkpoint['perplexities']

    print(f"Model and optimizer loaded. Resuming from epoch {start_epoch}.")
else:
    # No checkpoint found, initializing a new model
    print("No checkpoint found, starting training from scratch.")
    model = LSTM(num_emb=len(vocab),
                 output_size=len(vocab),
                 num_layers=num_layers,
                 hidden_size=hidden_size).to(device)

    # Initialize optimizer
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    start_epoch = 0  # Start from the first epoch
    train_losses = []
    test_losses = []
    perplexities = []
    print("No checkpoint found, starting training from scratch.")

# Print model summary
print("\nModel Architecture:")
print(model)

# Calculate total parameters
total_params = sum(p.numel() for p in model.parameters())
print(f"\nTotal parameters: {total_params:,}")

# Loss function
loss_fn = nn.CrossEntropyLoss()

# Training and Validation Setup for LSTM
# Training Loop
start_time = time.time()

# Initialize the progress bar with the correct range
epoch_bar = trange(start_epoch, nepochs, desc="Training Progress")

for epoch in epoch_bar:
    # Training phase
    model.train()
    train_loss = 0
    train_steps = 0

    batch_bar = tqdm(train_loader,
                    desc=f"Epoch {epoch+1}/{nepochs}",
                    leave=False,
                    ncols=100,
                    mininterval=1.0)

    for sequences, targets in batch_bar:
        sequences = sequences.to(device)
        targets = targets.to(device)
        bs = sequences.shape[0]
        hidden = torch.zeros(num_layers, bs, hidden_size, device=device)
        memory = torch.zeros(num_layers, bs, hidden_size, device=device)

        # Forward pass
        output, hidden, memory = model(sequences, hidden, memory)
        output = output.view(-1, len(vocab))
        targets = targets.view(-1)

        # Calculate loss
        loss = loss_fn(output, targets)
        train_loss += loss.item()
        train_steps += 1

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Update progress bar
        batch_bar.set_postfix(loss=f"{loss.item():.4f}")

    avg_train_loss = train_loss / train_steps
    train_losses.append(avg_train_loss)

    # Validation Phase
    model.eval()
    test_loss = 0
    test_steps = 0

    with torch.no_grad():
        for sequences, targets in tqdm(test_loader, desc="Validation", leave=False):
            sequences = sequences.to(device)
            targets = targets.to(device)
            bs = sequences.shape[0]
            hidden = torch.zeros(num_layers, bs, hidden_size, device=device)
            memory = torch.zeros(num_layers, bs, hidden_size, device=device)

            output, hidden, memory = model(sequences, hidden, memory)
            output = output.view(-1, len(vocab))
            targets = targets.view(-1)

            loss = loss_fn(output, targets)
            test_loss += loss.item()
            test_steps += 1

    avg_test_loss = test_loss / test_steps
    test_losses.append(avg_test_loss)

    # Calculate perplexity
    perplexity = calculate_perplexity(avg_test_loss)
    perplexities.append(perplexity)

    # Update progress bar
    epoch_bar.set_postfix(
        train_loss=f"{avg_train_loss:.4f}",
        test_loss=f"{avg_test_loss:.4f}",
        perplexity=f"{perplexity:.2f}"
    )

    # Generate sample text every few epochs
    if (epoch + 1) % 2 == 0:
        sample_text = generate_text(model, vocab, tokenizer, "Le président", max_length=20)
        print(f"\nSample text generation: {sample_text}\n")

    if (epoch + 1) % 5 == 0 or epoch == nepochs - 1:
        torch.save({
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'epoch': epoch,
            'train_losses': train_losses,
            'test_losses': test_losses,
            'perplexities': perplexities
        }, f"checkpoint_epoch_{epoch+1}.pth")
        print(f"Checkpoint saved for epoch {epoch+1}")
        
# Save the model and optimizer state
torch.save({
    'epoch': current_epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'train_losses': train_losses,
    'test_losses': test_losses,
    'perplexities': perplexities
}, 'model_checkpoint.pth')

print("Model and optimizer state saved.")

# Training Summary and Visualization
training_time = time.time() - start_time
print(f"\nTraining completed in {training_time:.2f} seconds")

plt.figure(figsize=(15, 5))

# Plot losses
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss')
plt.plot(test_losses, label='Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Test Loss')
plt.legend()

# Plot perplexity
plt.subplot(1, 2, 2)
plt.plot(perplexities, color='green')
plt.xlabel('Epoch')
plt.ylabel('Perplexity')
plt.title('Model Perplexity')
plt.tight_layout()
plt.show()